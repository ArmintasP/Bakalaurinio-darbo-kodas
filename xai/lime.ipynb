{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04af61ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models import VGG16_Weights\n",
    "\n",
    "from lime import lime_image\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from skimage.color import label2rgb\n",
    "\n",
    "\n",
    "device = torch.device('cuda')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "weights = VGG16_Weights.DEFAULT\n",
    "imagenet_classes = weights.meta['categories']\n",
    "\n",
    "model = models.vgg16(weights=weights).to(device)\n",
    "model.eval()\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def batch_predict(images):\n",
    "    model.eval()\n",
    "\n",
    "    batch = torch.stack([\n",
    "        preprocess(Image.fromarray(img)) for img in images\n",
    "    ]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(batch)\n",
    "        probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "    return probs.cpu().numpy() \n",
    "\n",
    "img_path = 'abba5.jpg' \n",
    "original_image = np.array(Image.open(img_path).convert('RGB'))\n",
    "\n",
    "explainer = lime_image.LimeImageExplainer()\n",
    "\n",
    "explanation = explainer.explain_instance(\n",
    "    original_image,\n",
    "    batch_predict,\n",
    "    top_labels=5,\n",
    "    num_samples=2000  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a26c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = explanation.segments  # 2D array, same size as image\n",
    "\n",
    "top_label = explanation.top_labels[0]\n",
    "weights = dict(explanation.local_exp[top_label])  # { superpixel_idx: weight }\n",
    "\n",
    "heatmap = np.zeros(segments.shape)\n",
    "for seg_idx in np.unique(segments):\n",
    "    heatmap[segments == seg_idx] = weights.get(seg_idx, 0)\n",
    "\n",
    "if np.max(heatmap) != np.min(heatmap):\n",
    "    heatmap_norm = (heatmap - np.min(heatmap)) / (np.max(heatmap) - np.min(heatmap))\n",
    "else:\n",
    "    heatmap_norm = heatmap\n",
    "\n",
    "vmax = np.max(np.abs(heatmap)) \n",
    "vmin = -vmax \n",
    "\n",
    "\n",
    "\n",
    "top_label_name = imagenet_classes[top_label]\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "# plt.imshow(original_image)\n",
    "plt.imshow(heatmap, cmap='bwr', alpha=0.5,vmin=vmin, vmax=vmax )  # red=positive, blue=negative\n",
    "plt.title(f\"LIME Heatmap (Strength) for {top_label_name}\")\n",
    "plt.axis('off')\n",
    "plt.colorbar(label='Importance Weight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a059699a",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_label = explanation.top_labels[0]\n",
    "temp, mask = explanation.get_image_and_mask(\n",
    "    top_label, positive_only=False, num_features=5, hide_rest=False\n",
    ")\n",
    "img_boundry = mark_boundaries(temp / 255.0, mask)\n",
    "\n",
    "plt.imshow(img_boundry)\n",
    "plt.title(f'LIME Explanation for class {top_label}')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lime-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
